---- Current tasks ---- X = Done, ? = Unsure, . = Partially complete
[X]  Define data set
        Use maia test set: December 2019. With their preprocessing. 
        Make training set: January 2019
        [X] Also run maia on other test sets: other to show it does generalize. 

[X]  Implement more chess heuristics 
        [X] Weight p-squared tables for each piece (Gabriel)
                [X] Make it possible to do any combination, say king and rest
        [X] Piece activity (log(avail_actions)) (Gabriel)
        [X] King safety (Nikolaj)
        [X] Pawn structure

[ ] Run BIRL model on sunfish moves (Gabriel)
        [ ] Plot accuracy, reward and energy over epochs
                [ ] For sunfish policy
                [ ] For ALpha-Beta policy

[.]  Do the comparison. Produce the plots:
        [.]      Validation of sunfish GRW method. Convergence plots (100 epochs) on Sunfish moves and player moves.
                [ ] Material and depth
                        [ ] 1100, 1900
                [ ] Material weighting (Fabian, Gabriel)
                        [ ] Using Alpha-Beta search (Gabriel)
                                [ ] On sunfish moves
                        [ ] Color code weight plots corretly
                        [X] Starting from 100, 100..
                                [X] Run 1100,1900 on player and sunfish moves
                                [X] Run 1300, 1500, 1700
                        [ ] From 1000
                                [ ] 1100, 1300, 1500, 1700, 1900 on player moves
                [ ] P-squared table (Split plots into material and pst values) (Gabriel, Fabian)
                        [ ] 2 values: material and pst
                                [ ] From Sunfish init
                                        [ ] ELO 1100, 1900
                        [ ] Varying all pst values, fixed material
                                [ ] 1100, 1300, 1900
                        [ ] Variable material, (pawn strucuture), king pst and rest of pst
                                [ ] 1100, 1900
                        [ ] All material, all pst
                                [ ] 1100, 1900
        [.]      Accuracy of Maia and Sunfish models at different ELO, trained on different ELO levels (Fabian)
                        Same as in Maia paper. With baselines.
                [X] Maia model only plot
                [X] Add Sunfish from 100, 100
                [ ] Add Sunfish from Sunfish init 
        [.]      Accuracy over ply. How many moves into the game. (Fabian)
                [X] Implement script to run plots
                [X] Script for combining into single plot
                [X] From 100,100...
                        [X] Run on models at 1100, 1900
                        [X] Run 1300, 1500, 1700
                [ ] From 1000
                        [ ] Run 1100, 1900, Default
                [ ] Using reversed data
                        [ ] 1100, 1900
        [ ]      Accuracy as function of training data set size (sunfish and maia) (Probably not, Gabriel)
                [ ] Set up Maia training 
                [ ] Set up Sunfish training
                [ ] Plot

Report:
[ ] Abstract
[.] Introduction (Each paragraph could be used as the first paragraph of each section)
        [.] Theory (IRL)
                [x] What is IRL
                [x] Imitation Learning Taxonomy
                [x] Soft intention inference intro
        [x] Learning objectives
                [x] Goal explained
                [x] Bullet points
                        [x] IRL litterature study
                        [x] Exploring IRL for chess domain
                        [x] Defining a dataset
                        [x] Replicating Maia chess results
                        [x] Developing a more interpretable player move prediction model
                        [x] Quantitatively characterising the performance of our method
                        [x] Qualitatively characterising the performance of our method
        [.] Method
                [.] Sunfish GRW explained
                [.] How learning objectives are solved
                [ ] Our method vs Maia
        [.] Results & Discussion
                [.] Difference of styles across ELOs
                        [.] More fidelity over Maia's un-explainable model (only possible through its moves, Cynthia Ruodin) 
                [.] Model performance
                        [x] Skill difference hard to show
                        [x] Depth difference detectable
                        [x] GRW validated
                                [x] Ground truth for pst-tables is achieved
                                [x] Material values ranked correctly
                [.] Weight intertpretation
                        [.] Pawn higher than expected
                        [.] Pieces ranked according to conventional wisdom (affine transform)
                        [x] Depth higher for higher ELO
                [ ] BIRL results!!
        [.] Conclusion
                [.] Interpretation of weights shows little difference
                [x] New baseline for player move prediction developed
                [x] Maia results replicated
                [x] Bug in Sunfish fixed
[.] Theory
        [.] Notation
                [X] What is a policy
                [ ] Figure of search tree
        [ ] IRL
                [.] Explain what it is
                        [x] Detailed explanation
                        [x] Difference from behavioural cloning
                        [x] Pseudo-code
                        [ ] Figure, what?
                [.] Describe main categories
                        [ ] Margin
                        [.] Entropy
                        [ ] Detailed characterization of BIRL. 
                                [ ] How our model relates to IRL. (Gabriel)
                                [ ] Policy Walk
                        [ ] GAN
                [.] Explain what methods are inapplicable to our domain and why
                        [.] Occupancy rates for NN
        [.] Behavioural cloning
                [.] Explanation
                [ ] Maia
        [.] Chess programming (Nikolaj)
                [.] Historical overview
                        [x] Simple engines
                        [x] Searching
                                [x] Deep blue
                                [x] Stockfish
                        [x] Neural Networks
                                [x] Stockfish
                                [x] Leela chess zero (Alpha Zero)
                                [x] Maia (Human like/Help humans improve)
                        [x] Sunfish
                                [x] Search strategy
                                [ ] Write pseudocode.
[ ] Methodology
        [ ] Chosing a chess framework
                [X] HM Alpha Beta Quiscent search
                [ ] Sunfish MTD-bi search
                        [x] Hyper-parameters
                                [x] Time, max-depth
                [x] Inner RL problem still expensive
        [ ] BIRL
                [ ] Implementation details
                        [ ] Hyper-parameters
                [ ] Why BIRL inflates energy causing reward function pertubation instability
                        [ ] Discuss lack of strong priors
                        [ ] Stochastic searcher leads to potentially different Q-values due to varying max-depth, q-value compared for different problems
                        [ ] Compare with alpha-beta with a constant max depth 
        [ ] Sunfish GRW
                [x] Implementation details
                [x] Non convex optimisation (many local optima)
        [x] Maia
                [x] Training process
                        [ ] Failed attempt, Deprication
                [x] Replication of Maia

[.] Data (Fabian)
        [X] Overview of online chess data in 2024
                [X] Plot amount of data available over time
        [X] Define valid positions
                [X] Time, ELO
                [X] Test/train split
        [X] Show distribution of [ELO, move ply] in train/test
                [X] In Maia test set
                [X] In 1100, 1900 ELO bin

[ ] Results
        [ ] For a specific position show what players at different ELOs do
                [ ] Using occupancy ratios 
                [ ] Show how different models 
                [ ] Use as example for all models
        [ ] BIRL plots
                [ ] pst/material over time
                [ ] Energies over time
                [ ] Accuracies over time
        [ ] Sunfish GRW plots
                [ ] Run convergence with time limit 0.2 seconds
                        [ ] 1100, 1900, sunfish/player moves, with/without pst
                [x] Search depth per ELO
                [x] Mention computational cost of plots
        [x] Maia plots
                [x] Per ELO (same as in paper, correctly replicated)
                [x] Per ply
                        [x] Put default Sunfish on 1100 and 1900 plots

[ ] Discussion
        [x] Computational costs of methods
        [.] Bayesian Optimisation Fail
        [ ] Player move accuracy predition
                [x] Large variance in data
                        [x] Using the average move
                [x] Maia results replicated correctly
                [x] Maia not that different from ELO to ELO
                [x] Sunfish the same across ELOs
                        [ ] Stockfish in Maia paper same accuracy as Sunfish 
        [ ] Why BIRL fails
                [ ] BIRL Priors are uniform
                [ ] Instability
                [ ] Parameterising reward function
                [ ] Why Leela chess feature vectors won't work (still fails)
        [ ] Sunfish Weight intertpretation
                [x] Pawn higher than expected
                [x] Pieces ranked according to conventional wisdom (affine transform)
                        [x] Position valued as expected (pawns+knights in focus)
                [x] Depth higher for higher ELO
        [x] Depth interpretation
        [.] Bridging the gap between symbolic and data driven AI to maintain explainability
                
[.] Conclusion
        [x] Interpretation of weights shows little difference
        [.] New baseline for player move prediction developed
        [x] Maia results replicated and are (XYZ)
        [x] Bug in Sunfish fixed
        [x] Future work
                [x] Look from specific opening 6 ply forward, so occupancy ratios work. 
                [x] Using Leela chess embeddings (Tue's words)

[ ]  Qualitlively compare key positions and blunder prediction, so we can discuss searching vs. not searching
[ ]     Show in demonstrations in jupyter notebooks.   


---- Less important notes
[ ]     Implement BO to make optimization more robust.
[ ]          Generate plots of optimization function to characterize it.
[ ]  Go through specific case studies
[ ]  Show in demonstrations in jupyter notebooks.   
[ ]  Implement more baselines to validate method

[.]    Go through Berkely Deep RL course
[.]    Read and understand reading list (see reading list section)


---- Tentative timeline ----
1st February: 
    Start date

1st March:
    Finish projektplan
    Complete Berkely Deep RL course
    Complete reading list and have strong overview of modern IRL methods
    Ideally having minimum working prototype 

1st April:
    Harmonize and standardize code library
    Implement more BIRL methods within the framework
    Complete Berkely Deep RL course
    Complete reading list and have strong overview of modern IRL methods

1st May:

1st June:
    Aim to be done. Send report to Tue. 

6th June:
    Submission deadline    


---- Report TODO ----
    - Write Theory
        - Introduce IRL.
            - Discuss various IRL methods. Limitations of visitation frequency. MAP. BIRL. Compare and contrast.
            - Say many of the same things and in Berkely video
        - Bayesian Optimization: Gaussian process
        - Chess Programming. 
    - Method

---- Relevant IRL papers reading list ----

    - Bayesian Inverse Reinforcement Learning (The crutch of our project so far)

Important/potentially project changing:
    - Deep Bayesian Reward Learning
    - Scalable Bayesian Inverse Reinforcement Learning
    - Survey of Inverse Reinforcement Learning

Less vital/useful background:
    - Algorithms for Inverse Reinforcement Learning (original Andrew Yang paper)


---- Questions for Tue ----
[X] Formlisation of IRL problem
        [ ] Chess RL defined, how to define IRL problem
[X] Chess programming role in report
        [X] History + tehcnical development, as will be included in discussion
        [ ] 
[X] What plots to include
        [X] How many are too many?
[X] How precisely does computational time have to be reported?
[X] How much detail to explain searching in Sunfish
        [ ] pseudo-code in report or appendix?

----- Notes from Tue -------

Presentér processen af projektet i IRL, mål om at lave IRL, endte med en noget anderledes approach

Indre RL loop med én iteration af policy iteration, inspireret IRL. Vi optimerer policyen ligesom en imitation learning, men med en reqrd vektor der kan fortolkes.

Vi skal flette alt ind vi har lavet, men de mere perifære ting skal præsenteres som sådan.

Ca træningstid er fint.

Stor introduktion, klare læringsobjektiver bliver defineret her - muligvis i bullet points.

Plots skal ikke være repetitive

Konklusionen skal holde sig tæt op af læringsmålene, og adressere dem. 

Outcome: baseline for IRL på skak, fixed bug i Sunfish, skak-heuristikker.

Helt fast på definitionerne af IRL og imitation learning i introduktionen.



