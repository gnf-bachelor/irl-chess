---- Current tasks ---- X = Done, ? = Unsure, . = Partially ocmplete

[ ]    Can always write on the report

[ ]    Sunfish top-k
[ ]    Sunfish depth as parameter (max depth of iterative deepening)
[ ]    Evaluate founds heuristics:
            More proofs of concept and affirmation that it works
            Characterization of players at different levels (bad, mid, good; also AI)
                Would be very cool with characterization of specific players. 

[.]    Go through Berkely Deep RL course
[.]    Read and understand reading list (see reading list section)

[ ]     Consider GAN like architecture with descriminator for policy moves and data moves 
            (optimize reward function to distinguish between policy moves and data moves,
            policy is then informed by that reward function. Sample from that. Gradient step again.)
                Figure out how this works and if we want to try it. 
            

[ ]    Use pst in evaluation function.
[ ]    Implement evaluation for king safety, pawn structure, etc.
[ ]    Implement self-play for sunfish vs sunfish with moderated eval function.

Tue ideer:
- greedy eval, glem 3.c else statement
- top k idé, for R og R_ hvor mange af trækkene i deres top k er i A? w+1/W+1
- Leela0 vektorer?  



---- Tentative timeline ----
1st February: 
    Start date

1st March:
    Finish projektplan
    Complete Berkely Deep RL course
    Complete reading list and have strong overview of modern IRL methods

    Ideally having minimum working prototype

1st April:
    Harmonize and standardize code library - Almost there

1st May:
    Implement more BIRL methods within the framework
    Implement k-best moves in Sunfish as well (Gabriel) (My framework with priority queue)
    Complete Berkely Deep RL course
    Complete reading list and have strong overview of modern IRL methods

16th May - 31th May : Bachelor grind period

31th May:
    Be ready to submit

6th June:
    Submission deadline    


---- Report TODO ----
    - Write Theory
        - Introduce RL. Including formal introduction with nomanclature and math.
        - Introduce IRL.
            - Discuss various IRL methods. Limitations of visitation frequency. MAP. BIRL. Compare and contrast.
            - Say many of the same things and in Berkely video
        - Bayesian Optimization: Gaussian process
        - Chess Programming. 
    - Method
        - Sketch what we aim to do. We will of course change this later. 

---- Relevant IRL papers reading list ----

    - Bayesian Inverse Reinforcement Learning (The crutch of our project so far)

Important/potentially project changing:
    - Deep Bayesian Reward Learning
    - Scalable Bayesian Inverse Reinforcement Learning
    - Survey of Inverse Reinforcement Learning

Less vital/useful background:
    - Algorithms for Inverse Reinforcement Learning (original Andrew Yang paper)


---- Questions for Tue ----
    - Discuss trajectory from here. Other approaches?
        - Minimum viable prototype?
        - GAN architecture. Make Network that tries to maximize difference between performance of data and most recent trained policy. 
            - Ideas from Tue of representation and feasibility. 
        - MAP estimation instead of bayesian approach.
    - Thoughts on whether model trajectories are optimal enough with respect to any reward function we can reasonably model?

