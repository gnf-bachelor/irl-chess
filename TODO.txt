---- Current tasks ---- X = Done, ? = Unsure, . = Partially complete
[ ]  Define data set
        Use maia test set: December 2019. With their preprocessing. 
        Make trainings set: January 2019
        Also run maia on other test sets: other to show it does not generalize. 

[ ]  Implement more chess heuristics, including weighing pst, as parameters in the model. Weigh this against observed performance.
[ ]     Implement BO to make optimization more robust.
[ ]          Generate plots of optimization function to characterize it.

[ ]  Do the comparison. Produce the plots:
[ ]      Validation of sunfish method. Convergence plots on Sunfish moves and player moves. 
[ ]      Accuracy of Maia and Sunfish models at different ELO, trained on different ELO levels
                Same as in Maia paper. With baselines.
[ ]      Accuracy over ply. How many moves into the game. 
[ ]      Accuracy as function of training data set size (sunfish and maia)

[ ]  Detailed characterization of Bayesian IRL. How our model relates to IRL.
[ ]  Why do other methods not work.
[ ]  If others work, implement them. Look from specific opening 6 ply forward, so occupancy ratios work. 

[ ]  Qualitlively compare key positions and blunder prediction, so we can discuss searching vs. not searching
[ ]     Show in demonstrations in jupyter notebooks.   


---- Less important notes
[ ]  Discussion: Bridging the gap between symbolic and data driven AI to maintain explainability
[ ]  Go through specific case studies
[ ]  Show in demonstrations in jupyter notebooks.   
[ ]  Implement more baselines to validate method

[.]    Go through Berkely Deep RL course
[.]    Read and understand reading list (see reading list section)



[ ]    Run the following experiments for actual results:
        - Sunfish moves
            - Lower vs higher ELO
                - Sunfish
                - (Maia?)
        - Player game moves
            - Lower vs higher ELO
                - Sunfish
                - Maia
            - Test on player games
        - Player puzzle moves
            - Lower vs higher ELO
                - Sunfish
                - Maia
            - Test on player games





---- Tentative timeline ----
1st February: 
    Start date

1st March:
    Finish projektplan
    Complete Berkely Deep RL course
    Complete reading list and have strong overview of modern IRL methods
    Ideally having minimum working prototype 

1st April:
    Harmonize and standardize code library
    Implement more BIRL methods within the framework
    Complete Berkely Deep RL course
    Complete reading list and have strong overview of modern IRL methods

1st May:

1st June:
    Aim to be done. Send report to Tue. 

6th June:
    Submission deadline    


---- Report TODO ----
    - Write Theory
        - Introduce IRL.
            - Discuss various IRL methods. Limitations of visitation frequency. MAP. BIRL. Compare and contrast.
            - Say many of the same things and in Berkely video
        - Bayesian Optimization: Gaussian process
        - Chess Programming. 
    - Method

---- Relevant IRL papers reading list ----

    - Bayesian Inverse Reinforcement Learning (The crutch of our project so far)

Important/potentially project changing:
    - Deep Bayesian Reward Learning
    - Scalable Bayesian Inverse Reinforcement Learning
    - Survey of Inverse Reinforcement Learning

Less vital/useful background:
    - Algorithms for Inverse Reinforcement Learning (original Andrew Yang paper)


---- Questions for Tue ----

