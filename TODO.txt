---- Current tasks ---- X = Done, ? = Unsure, . = Partially ocmplete
[x]    Get sunfish up and running.
[x]        Write trainable alpha-beta search function weighted with BIRL heuristics.
[X]        Combine the two by permuting the weights and using the function to approximate the true ones
[?]            Evaluate performance (doesn't yeild promising results)
[ ]                Evaluate heuristics qualitatively for different trained chess engines.

[X]    Segment Lichess dataset and make it amenable to Bayesian IRL algorithm.
[X]        Begin with a small sample.
[.]    Go through Berkely Deep RL course
[.]    Read and understand reading list (see reading list section)

[ ]     Consider GAN like architecture with descriminator for policy moves and data moves 
            (optimize reward function to distinguish between policy moves and data moves,
            policy is then informed by that reward function. Sample from that. Gradient step again.)
                Figure out how this works and if we want to try it. 
            

[ ]    Use pst in evaluation function.
[ ]    Implement evaluation for king safety, pawn structure, etc.
[ ]    Implement self-play for sunfish vs sunfish with moderated eval function.




---- Tentative timeline ----
1st February: 
    Start date

1st March:
    Finish projektplan
    Complete Berkely Deep RL course
    Complete reading list and have strong overview of modern IRL methods

    Ideally having minimum working prototype 

1st April:
    Harmonize and standardize code library
    Implement more BIRL methods within the framework
    Complete Berkely Deep RL course
    Complete reading list and have strong overview of modern IRL methods


1st May:


6th June:
    Submission deadline    


---- Report TODO ----
    - Write Theory
        - Introduce RL. Including formal introduction with nomanclature and math.
        - Introduce IRL.
            - Discuss various IRL methods. Limitations of visitation frequency. MAP. BIRL. Compare and contrast.
            - Say many of the same things and in Berkely video
        - Bayesian Optimization: Gaussian process
        - Chess Programming. 
    - Method
        - Sketch what we aim to do. We will of course change this later. 

---- Relevant IRL papers reading list ----

    - Bayesian Inverse Reinforcement Learning (The crutch of our project so far)

Important/potentially project changing:
    - Deep Bayesian Reward Learning
    - Scalable Bayesian Inverse Reinforcement Learning
    - Survey of Inverse Reinforcement Learning

Less vital/useful background:
    - Algorithms for Inverse Reinforcement Learning (original Andrew Yang paper)


---- Questions for Tue ----
    - Discuss trajectory from here. Other approaches?
        - Minimum viable prototype?
        - GAN architecture. Make Network that tries to maximize difference between performance of data and most recent trained policy. 
            - Ideas from Tue of representation and feasibility. 
        - MAP estimation instead of bayesian approach.
    - Thoughts on whether model trajectories are optimal enough with respect to any reward function we can reasonably model?

