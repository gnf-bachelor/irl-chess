\section*{Project Plan}
\subsection*{Bayesian inverse reinforcement learning for inferring player evaluation of chess positions}

When modelling human actors as autonomous agents, it is often exceedingly difficult to generalize past any given policy and capture the intent of the task at hand. The task objective might be easily explained using the rich semantics and context of language and human experience, but how to dynamically learn this from data and demonstrations is subject of much study and has given rise to the field of Inverse Reinforcement Learning (IRL): the problem of inferring the reward function of an agent, given its policy or observed behavior\cite{IRL_survey}. 

This project will examine IRL in the context of chess. We observe that players of different skill levels evaluate the position differently, and if many games of players of similar skill are grouped together this defines an inverse reinforcement learning problem where the goal is to learn what the players value. We are interested in exploring IRL's potential to infer the intention of human actors when it is obtuse even to themselves.  

The project will parameterize the reward function as a linear combination of common chess heuristics, such as material value, king safety, piece activity, pawn structure, etc. to maintain interpretability.
First the project will validate that the premise is sound by implementing a random walk over reward functions. This method will iteratively perturb the reward function, loop over the positions and moves taken in the dataset and compare how often the observed move from the data is in the top k moves generated by alpha-beta searching using the two reward functions and then greedily accept the reward function that most often contains the observed move. Thus if the premise holds any water, this random walk should converge a reasonable chess reward function.  

Then we will examine an approach inspired by the method “Bayesian Inverse Reinforcement Learning”\cite{Bayesian IRL}. This method derives a posterior over Reward functions and demonstrates a method which can effectively sample from this posterior using MCMC sampling. However, the method is limited to systems where the states and actions can be enumerated and each step of the method requires a full evaluation of policy iteration – this makes the method computationally infeasible for chess and other non-trivial problems.                   
This project will amend this approach, by parameterizing the Q-functions with a shallow alpha-beta search using a weak search engine, circumventing the by far most computationally intensive step at the cost of optimality, but maintaining the ability to derive a posterior over reward functions.
As such this project will describe the limitations of this approach, and apply the modified method to real-world data obtained from the open-source chess website Lichess. Based on (weak) human players, the project will examine whether reward functions learned on players of different strength can 1. be accurately ascertained and 2. distinguish between high and low-skill players, and then discuss whether high or low-skill players exhibit different preferences in terms of how they evaluate board positions.

The project will further examine alternative MAP based IRL approaches.
Finally, the project will compare and contrast all examined approaches in the context of IRL and this problem specifically, including examing the resulting (posterior) estimates of the reward function obtained by the BIRL method. It will discuss whether the methods find meaningful differences between how players value material at different skill levels, whether the modified BIRL method provides sensible uncertainty estimates and discuss the limitions of each of these approaches. 




The project will:
Adapt the BIRL method to the problem of chess
Discuss limitations of the proposed approach and contrast these with BIRL and other more recent approaches to IRL that relies on MAP estimation of the reward function
Define a dataset of Lichess games from players of different skill levels
Train the proposed method and validate the method using objective criteria such as prediction of next moves
Examine the resulting (posterior) estimates of the reward function obtained by the method. Discuss whether the method finds meaningful differences between how players value material at different skill levels and whether the modified method provides sensible uncertainty estimates.